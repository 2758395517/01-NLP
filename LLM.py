# LLM.py - ä¿®å¤ç‰ˆï¼ˆè°ƒæ•´ç•Œé¢å¸ƒå±€ï¼‰
import json
import numpy as np
from typing import List, Dict, Optional
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
from database import get_vector_database
import re
import os

# è®¾ç½®ä½¿ç”¨å›½å†…é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'


class ImprovedMedicalAssistant:
    def __init__(self,
                 llm_model="Qwen/Qwen2.5-1.5B-Instruct",
                 use_gpu=False):

        print(f"åˆå§‹åŒ–åŒ»ç–—åŠ©æ‰‹ï¼Œä½¿ç”¨æ¨¡å‹: {llm_model}")

        # åŠ è½½å‘é‡æ•°æ®åº“
        self.vector_db = get_vector_database()
        if self.vector_db is None:
            print("è­¦å‘Š: å‘é‡æ•°æ®åº“åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨å›é€€æ¨¡å¼")

        # åŠ è½½LLM
        self.device = "cuda" if torch.cuda.is_available() and use_gpu else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")

        try:
            # åŠ è½½tokenizerå’Œmodel
            self.tokenizer = AutoTokenizer.from_pretrained(
                llm_model,
                trust_remote_code=True,
                padding_side="left"
            )

            self.model = AutoModelForCausalLM.from_pretrained(
                llm_model,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None,
                trust_remote_code=True
            )

            # åˆ›å»ºpipeline
            self.pipe = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if self.device == "cuda" else -1,
                max_new_tokens=512,
                temperature=0.3,
                do_sample=True,
                top_p=0.9,
                repetition_penalty=1.1
            )

            self.llm_loaded = True
            print("LLMåŠ è½½æˆåŠŸ")

        except Exception as e:
            print(f"LLMåŠ è½½å¤±è´¥: {e}")
            print("å°†ä½¿ç”¨åŸºäºè§„åˆ™çš„å›å¤")
            self.llm_loaded = False

        # å¯¹è¯å†å²
        self.conversation_history = []
        self.max_history = 5

    def retrieve_relevant_info(self, query: str, top_k: int = 3) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³ä¿¡æ¯ - ä¿®å¤ç‰ˆï¼šä»æ–‡æœ¬ä¸­æå–é—®é¢˜å’Œç­”æ¡ˆ
        """
        if self.vector_db is None:
            return []

        try:
            results = self.vector_db.search(query, top_k=top_k, threshold=0.5)

            # è°ƒè¯•ä¿¡æ¯
            print(f"\n[æ£€ç´¢] æŸ¥è¯¢: '{query}'")
            print(f"[æ£€ç´¢] æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")

            # è¿‡æ»¤å’Œæå–ä¿¡æ¯
            filtered_results = []
            for i, result in enumerate(results):
                if result['score'] > 0.5:
                    # å°è¯•ä»ä¸åŒä½ç½®æå–é—®é¢˜å’Œç­”æ¡ˆ
                    question = ""
                    answer = ""

                    # æ–¹æ³•1: ç›´æ¥ä»resultä¸­è·å–
                    if 'question' in result and result['question']:
                        question = result['question']
                    if 'answer' in result and result['answer']:
                        answer = result['answer']

                    # æ–¹æ³•2: ä»metadataä¸­è·å–
                    if (not question or not answer) and 'metadata' in result:
                        metadata = result['metadata']
                        if 'question' in metadata and metadata['question']:
                            question = metadata['question']
                        if 'answer' in metadata and metadata['answer']:
                            answer = metadata['answer']

                    # æ–¹æ³•3: ä»textå­—æ®µè§£æ
                    if (not question or not answer) and 'text' in result:
                        text = result['text']
                        # å°è¯•è§£æ "é—®é¢˜ï¼šxxx ç­”æ¡ˆï¼šxxx" æ ¼å¼
                        if "é—®é¢˜ï¼š" in text and "ç­”æ¡ˆï¼š" in text:
                            parts = text.split("ç­”æ¡ˆï¼š", 1)
                            if len(parts) > 1:
                                question_part = parts[0].replace("é—®é¢˜ï¼š", "").strip()
                                answer = parts[1].strip()
                                if not question:
                                    question = question_part

                    # æ–¹æ³•4: ä»chunkä¸­è·å–
                    if (not question or not answer) and 'chunk' in result:
                        chunk = result['chunk']
                        if 'text' in chunk:
                            text = chunk['text']
                            if "é—®é¢˜ï¼š" in text and "ç­”æ¡ˆï¼š" in text:
                                parts = text.split("ç­”æ¡ˆï¼š", 1)
                                if len(parts) > 1:
                                    question_part = parts[0].replace("é—®é¢˜ï¼š", "").strip()
                                    answer = parts[1].strip()
                                    if not question:
                                        question = question_part

                        if 'content' in chunk and not answer:
                            answer = chunk['content']

                    # å¦‚æœä»ç„¶æ²¡æœ‰ç­”æ¡ˆï¼Œä½¿ç”¨textçš„å‰100ä¸ªå­—ç¬¦
                    if not answer and 'text' in result:
                        answer = result['text'][:100]

                    # å¦‚æœä»ç„¶æ²¡æœ‰é—®é¢˜ï¼Œä½¿ç”¨æŸ¥è¯¢æˆ–ç•™ç©º
                    if not question:
                        question = query[:50]

                    # æ¸…ç†ç­”æ¡ˆ
                    if answer:
                        # ç§»é™¤å¯èƒ½çš„"ç­”æ¡ˆï¼š"å‰ç¼€
                        if answer.startswith("ç­”æ¡ˆï¼š"):
                            answer = answer[3:].strip()

                        # é™åˆ¶é•¿åº¦
                        if len(answer) > 200:
                            answer = answer[:200] + "..."

                    print(f"[ç»“æœ{i + 1}] åˆ†æ•°: {result['score']:.3f}, é—®é¢˜: '{question[:30]}...', ç­”æ¡ˆ: '{answer[:30]}...'")

                    filtered_results.append({
                        'question': question,
                        'answer': answer,
                        'department': result.get('department', ''),
                        'score': result['score']
                    })

            print(f"[è¿‡æ»¤] æœ€ç»ˆä¿ç•™ {len(filtered_results)} ä¸ªç»“æœ")
            return filtered_results

        except Exception as e:
            print(f"æ£€ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []

    def create_prompt(self, query: str, context: List[Dict], history: List[str]) -> str:
        """
        åˆ›å»ºæç¤ºè¯ - ç®€åŒ–ç‰ˆ
        """
        # ç³»ç»Ÿæç¤º
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ï¼Œè¯·ç”¨ä¸­æ–‡ç®€æ´å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä»¥ä¸‹ä¿¡æ¯æœ‰ç”¨ï¼Œè¯·åŸºäºä¿¡æ¯å›ç­”ï¼Œå¦åˆ™æ ¹æ®ä½ çš„çŸ¥è¯†å›ç­”ã€‚
æœ€åæé†’ç”¨æˆ·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚"""

        # æ·»åŠ ä¸Šä¸‹æ–‡
        context_text = ""
        if context:
            context_text = "\n\nå‚è€ƒä¿¡æ¯ï¼š"
            for i, ctx in enumerate(context, 1):
                context_text += f"\n{i}. {ctx['answer']}"

        # å®Œæ•´æç¤º
        full_prompt = f"""{system_prompt}{context_text}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·ç›´æ¥ç»™å‡ºç­”æ¡ˆï¼š"""

        return full_prompt

    # åœ¨ generate_answer_with_llm æ–¹æ³•ä¸­ä¿®æ”¹
    def generate_answer_with_llm(self, prompt: str) -> str:
        """ä½¿ç”¨LLMç”Ÿæˆå›ç­” - ä¿®å¤é‡å¤é—®é¢˜"""
        if not self.llm_loaded:
            return "ç³»ç»Ÿæ­£åœ¨ç»´æŠ¤ä¸­ï¼Œè¯·ç¨åå†è¯•ã€‚"

        try:
            # æ·»åŠ ç”Ÿæˆå‚æ•°ï¼Œé˜²æ­¢é‡å¤
            generation_config = {
                "max_new_tokens": 512,
                "num_return_sequences": 1,
                "pad_token_id": self.tokenizer.eos_token_id,
                "truncation": True,
                "temperature": 0.3,
                "do_sample": True,
                "top_p": 0.9,
                "repetition_penalty": 1.2,  # å¢åŠ é‡å¤æƒ©ç½š
                "no_repeat_ngram_size": 3,  # é˜²æ­¢3-gramé‡å¤
            }

            outputs = self.pipe(
                prompt,
                **generation_config
            )

            generated_text = outputs[0]['generated_text']

            # æå–å›ç­”éƒ¨åˆ†ï¼ˆå»æ‰promptï¼‰
            answer = generated_text[len(prompt):].strip()

            # å¼ºåŒ–æ¸…ç†é€»è¾‘
            answer = self.clean_answer(answer)

            return answer

        except Exception as e:
            print(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"

    # ä¿®æ”¹ clean_answer æ–¹æ³•
    def clean_answer(self, text: str) -> str:
        """æ¸…ç†å›ç­”æ–‡æœ¬ - ä¿®å¤é‡å¤é—®é¢˜"""
        if not text:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # 1. ç§»é™¤é‡å¤çš„å¥å­æˆ–æ®µè½
        import re
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]+', text)
        unique_sentences = []
        seen_sentences = set()

        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue

            # æ£€æŸ¥å¥å­æ˜¯å¦ç›¸ä¼¼ï¼ˆç®€å•å»é‡ï¼‰
            simplified = sentence.replace(" ", "").replace("ï¼Œ", "").replace("ã€‚", "")
            if simplified not in seen_sentences and len(sentence) > 3:
                seen_sentences.add(simplified)
                unique_sentences.append(sentence)

        # 2. é™åˆ¶æ€»å¥å­æ•°
        if len(unique_sentences) > 8:
            # ä¿ç•™é‡è¦éƒ¨åˆ†ï¼Œç§»é™¤åé¢çš„é‡å¤éƒ¨åˆ†
            important_keywords = ["è¯ç‰©", "ç”¨è¯", "å‰‚é‡", "å»ºè®®", "æ³¨æ„"]
            important_sentences = []
            other_sentences = []

            for sentence in unique_sentences:
                if any(keyword in sentence for keyword in important_keywords):
                    important_sentences.append(sentence)
                else:
                    other_sentences.append(sentence)

            # ä¼˜å…ˆä¿ç•™é‡è¦å¥å­ï¼Œé™åˆ¶æ€»é•¿åº¦
            if len(important_sentences) >= 5:
                unique_sentences = important_sentences[:5]
            else:
                unique_sentences = important_sentences + other_sentences[:5 - len(important_sentences)]

        cleaned_text = "ã€‚".join(unique_sentences)
        if cleaned_text and not cleaned_text.endswith("ã€‚"):
            cleaned_text += "ã€‚"

        # 3. ç¡®ä¿åªæœ‰ä¸€ä¸ªå…è´£å£°æ˜
        if "ä»…ä¾›å‚è€ƒ" in cleaned_text:
            # ç§»é™¤å¤šä½™çš„å…è´£å£°æ˜
            cleaned_text = re.sub(r'ä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒ[^\nã€‚]*[ã€‚\n]', '', cleaned_text, flags=re.DOTALL)
            cleaned_text = re.sub(r'è¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ[^\nã€‚]*[ã€‚\n]', '', cleaned_text, flags=re.DOTALL)
            # åœ¨æœ«å°¾æ·»åŠ ä¸€ä¸ªå¹²å‡€çš„å…è´£å£°æ˜
            if not cleaned_text.endswith("ã€‚") and not cleaned_text.endswith("."):
                cleaned_text += "ã€‚"
            cleaned_text += "\n\nâš ï¸ ä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ï¼Œè¯·å’¨è¯¢åŒ»ç”Ÿã€‚"
        else:
            cleaned_text += "\n\nâš ï¸ ä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œä¸èƒ½æ›¿ä»£ä¸“ä¸šåŒ»ç–—å»ºè®®ï¼Œè¯·å’¨è¯¢åŒ»ç”Ÿã€‚"

        return cleaned_text

    def answer_question(self,
                        query: str,
                        use_rag: bool = True,
                        include_references: bool = True) -> Dict:
        """
        å›ç­”ç”¨æˆ·é—®é¢˜ - ä¿®å¤ç‰ˆ
        """
        print(f"\n[å¤„ç†] é—®é¢˜: {query}")

        # æ£€ç´¢ç›¸å…³ä¿¡æ¯
        context = []
        if use_rag and self.vector_db is not None:
            context = self.retrieve_relevant_info(query, top_k=3)

        # ç”Ÿæˆå›ç­”
        if self.llm_loaded and context:
            # ä½¿ç”¨LLM + RAG
            prompt = self.create_prompt(query, context, self.conversation_history)
            answer = self.generate_answer_with_llm(prompt)
        elif context:
            # åªæœ‰RAGï¼Œæ²¡æœ‰LLM
            answer = self.generate_answer_from_context(query, context)
        else:
            # å›é€€æ¨¡å¼
            answer = self.generate_fallback_answer(query)

        # æ›´æ–°å¯¹è¯å†å²
        self.conversation_history.append((query, answer))
        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]

        # å‡†å¤‡è¿”å›ç»“æœ
        result = {
            "question": query,
            "answer": answer,
            "has_references": len(context) > 0,
            "retrieved_count": len(context)
        }

        # æ·»åŠ å‚è€ƒä¿¡æ¯
        if include_references and context:
            result["references"] = []
            for i, ref in enumerate(context, 1):
                result["references"].append({
                    "åºå·": i,
                    "ç›¸å…³æ€§": f"{ref['score']:.3f}",
                    "ç§‘å®¤": ref.get('department', 'æœªçŸ¥'),
                    "å‚è€ƒé—®é¢˜": ref['question'][:30] + ("..." if len(ref['question']) > 30 else ""),
                    "å‚è€ƒå†…å®¹": ref['answer'][:50] + ("..." if len(ref['answer']) > 50 else "")
                })

        print(f"[å®Œæˆ] å›ç­”é•¿åº¦: {len(answer)} å­—ç¬¦")
        return result

    def generate_answer_from_context(self, query: str, context: List[Dict]) -> str:
        """
        ç›´æ¥ä»ä¸Šä¸‹æ–‡ä¸­ç”Ÿæˆå›ç­”
        """
        if not context:
            return self.generate_fallback_answer(query)

        # ä½¿ç”¨æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡
        best_context = context[0]

        answer = f"æ ¹æ®åŒ»ç–—ä¿¡æ¯åº“ï¼š\n\n"
        answer += f"{best_context['answer']}\n\n"

        if len(context) > 1:
            answer += "å…¶ä»–ç›¸å…³ä¿¡æ¯ï¼š\n"
            for i, ctx in enumerate(context[1:3], 2):
                answer += f"{i}. {ctx['answer'][:50]}...\n"

        answer += "\nâš ï¸ ä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œå…·ä½“æƒ…å†µè¯·å’¨è¯¢åŒ»ç”Ÿã€‚"
        return answer

    def generate_fallback_answer(self, query: str) -> str:
        """
        ç”Ÿæˆå›é€€å›ç­”
        """
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        fallback_responses = {
            "é«˜è¡€å‹": """ğŸ’Š **é«˜è¡€å‹ç®¡ç†å»ºè®®**ï¼š
1. **è¯ç‰©æ²»ç–—**ï¼šéœ€åŒ»ç”Ÿè¯„ä¼°åé€‰æ‹©åˆé€‚çš„é™å‹è¯
2. **ç”Ÿæ´»æ–¹å¼**ï¼šä½ç›é¥®é£Ÿã€è§„å¾‹è¿åŠ¨ã€æ§åˆ¶ä½“é‡
3. **ç›‘æµ‹**ï¼šå®šæœŸæµ‹é‡è¡€å‹ï¼Œè®°å½•å˜åŒ–

ğŸ“Œ **æ³¨æ„**ï¼šå…·ä½“ç”¨è¯æ–¹æ¡ˆéœ€åŒ»ç”Ÿæ ¹æ®ç—…æƒ…åˆ¶å®šã€‚""",

            "ç³–å°¿ç—…": """ğŸ’‰ **ç³–å°¿ç—…ç®¡ç†è¦ç‚¹**ï¼š
1. **è¡€ç³–æ§åˆ¶**ï¼šå®šæœŸç›‘æµ‹è¡€ç³–
2. **è¯ç‰©æ²»ç–—**ï¼šå£æœé™ç³–è¯æˆ–èƒ°å²›ç´ 
3. **é¥®é£Ÿæ§åˆ¶**ï¼šé™åˆ¶ç¢³æ°´åŒ–åˆç‰©ï¼Œå¤šåƒè”¬èœ
4. **è¿åŠ¨**ï¼šæ¯å‘¨è‡³å°‘150åˆ†é’Ÿä¸­ç­‰å¼ºåº¦è¿åŠ¨

ğŸ“Œ **æ³¨æ„**ï¼šè¡€ç³–æ§åˆ¶ç›®æ ‡å› äººè€Œå¼‚ã€‚""",

            "æ„Ÿå†’": """ğŸ¤§ **æ„Ÿå†’å¤„ç†å»ºè®®**ï¼š
1. **ä¼‘æ¯**ï¼šä¿è¯å……è¶³ç¡çœ 
2. **è¡¥æ°´**ï¼šå¤šå–æ¸©æ°´
3. **å¯¹ç—‡ç”¨è¯**ï¼šè§£çƒ­é•‡ç—›è¯ç¼“è§£ç—‡çŠ¶
4. **å°±åŒ»**ï¼šå¦‚é«˜çƒ­ä¸é€€æˆ–ç—‡çŠ¶åŠ é‡ï¼Œè¯·åŠæ—¶å°±åŒ»

ğŸ“Œ **æ³¨æ„**ï¼šæ™®é€šæ„Ÿå†’å¤šä¸ºç—…æ¯’æ„ŸæŸ“ï¼ŒæŠ—ç”Ÿç´ æ— æ•ˆã€‚""",

            "èƒƒç—›": """ğŸ¤¢ **èƒƒç—›å¤„ç†å»ºè®®**ï¼š
1. **é¥®é£Ÿ**ï¼šæ¸…æ·¡æ˜“æ¶ˆåŒ–ï¼Œé¿å…è¾›è¾£åˆºæ¿€
2. **è¯ç‰©**ï¼šå¯è€ƒè™‘èƒƒé»è†œä¿æŠ¤å‰‚æˆ–æŠ—é…¸è¯
3. **å°±åŒ»**ï¼šå¦‚ç–¼ç—›æŒç»­ï¼Œè¯·å’¨è¯¢æ¶ˆåŒ–å†…ç§‘åŒ»ç”Ÿ

ğŸ“Œ **æ³¨æ„**ï¼šèƒƒç—›å¯èƒ½ç”±å¤šç§åŸå› å¼•èµ·ï¼Œéœ€æ˜ç¡®è¯Šæ–­ã€‚"""
        }

        for keyword, response in fallback_responses.items():
            if keyword in query:
                return response

        return "è¿™æ˜¯ä¸€ä¸ªåŒ»ç–—ç›¸å…³é—®é¢˜ã€‚å»ºè®®å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿè·å–å‡†ç¡®è¯Šæ–­ã€‚"

    def clear_history(self):
        """æ¸…ç©ºå¯¹è¯å†å²"""
        self.conversation_history = []


def create_interface():
    """åˆ›å»ºGradioç•Œé¢ - æ–°ç‰ˆå¸ƒå±€ï¼šå¯¹è¯æ¡†åœ¨ä¸Šï¼Œæ£€ç´¢ç»“æœåœ¨ä¸‹"""
    assistant = ImprovedMedicalAssistant()

    def respond(message, history, use_rag):
        if not message.strip():
            return history, "", []

        # è·å–å›ç­”
        result = assistant.answer_question(message, use_rag=use_rag)

        # æ›´æ–°èŠå¤©å†å²
        history.append((message, result["answer"]))

        # å‡†å¤‡å‚è€ƒä¿¡æ¯ - ä¿®å¤æ ¼å¼é—®é¢˜
        references = []
        if "references" in result and result["references"]:
            # è½¬æ¢ä¸ºäºŒç»´åˆ—è¡¨æ ¼å¼
            for ref in result["references"]:
                references.append([
                    ref.get("åºå·", ""),
                    ref.get("ç›¸å…³æ€§", ""),
                    ref.get("ç§‘å®¤", ""),
                    ref.get("å‚è€ƒé—®é¢˜", ""),
                    ref.get("å‚è€ƒå†…å®¹", "")
                ])

        print(f"[ç•Œé¢] å‘é€å›ç­”ï¼Œå‚è€ƒä¿¡æ¯æ•°é‡: {len(references)}")
        return history, "", references

    def clear_chat():
        assistant.clear_history()
        return [], []

    with gr.Blocks(title="åŒ»ç–—é—®ç­”ç³»ç»Ÿ", theme=gr.themes.Soft()) as demo:
        gr.Markdown("""
        # ğŸ¥ åŒ»ç–—é—®ç­”ç³»ç»Ÿ
        """)

        # ========== ç¬¬ä¸€éƒ¨åˆ†ï¼šå¯¹è¯æ¡†å’Œè¾“å…¥ ==========
        with gr.Row():
            chatbot = gr.Chatbot(
                height=400,
                bubble_full_width=True,
                show_copy_button=True,
                label="åŒ»ç–—é—®ç­”å¯¹è¯"
            )

        with gr.Row():
            user_input = gr.Textbox(
                placeholder="è¯·è¾“å…¥åŒ»ç–—é—®é¢˜ï¼Œå¦‚ï¼šé«˜è¡€å‹åƒä»€ä¹ˆè¯ï¼Ÿ",
                lines=3,
                label="è¾“å…¥é—®é¢˜",
                scale=4
            )

            with gr.Column(scale=1):
                submit_btn = gr.Button("å‘é€", variant="primary", size="lg")
                clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", variant="secondary")

        with gr.Row():
            rag_toggle = gr.Checkbox(
                label="å¯ç”¨æ™ºèƒ½æ£€ç´¢(RAG)",
                value=True,
                info="å¯ç”¨åç³»ç»Ÿä¼šä»åŒ»ç–—çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯"
            )

        gr.Markdown("---")  # åˆ†éš”çº¿

        # ========== ç¬¬äºŒéƒ¨åˆ†ï¼šæ£€ç´¢ç»“æœ ==========
        with gr.Row():
            gr.Markdown("### ğŸ” æ£€ç´¢ç»“æœ")

        with gr.Row():
            references_display = gr.Dataframe(
                headers=["åºå·", "ç›¸å…³æ€§", "ç§‘å®¤", "å‚è€ƒé—®é¢˜", "å‚è€ƒå†…å®¹"],
                datatype=["str", "str", "str", "str", "str"],
                height=300,
                wrap=True,
                interactive=False,
                label="æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯"
            )

        # çŠ¶æ€ä¿¡æ¯
        with gr.Row():
            status_info = gr.Markdown(
                "**ç³»ç»ŸçŠ¶æ€**: å°±ç»ª | **çŸ¥è¯†åº“**: 610,742æ¡æ•°æ® | **æ¨¡å‹**: Qwen2.5-1.5B"
            )

        # è®¾ç½®äº‹ä»¶å¤„ç†
        submit_btn.click(
            respond,
            inputs=[user_input, chatbot, rag_toggle],
            outputs=[chatbot, user_input, references_display]
        )

        user_input.submit(
            respond,
            inputs=[user_input, chatbot, rag_toggle],
            outputs=[chatbot, user_input, references_display]
        )

        clear_btn.click(
            clear_chat,
            outputs=[chatbot, references_display]
        )

        # æ·»åŠ ä¸€äº›ä½¿ç”¨æç¤º
        with gr.Accordion("ğŸ’¡ ä½¿ç”¨æç¤º", open=False):
            gr.Markdown("""
            1. **è¾“å…¥é—®é¢˜**ï¼šåœ¨ä¸‹æ–¹è¾“å…¥æ¡†è¾“å…¥æ‚¨çš„åŒ»ç–—é—®é¢˜
            2. **å‘é€æ–¹å¼**ï¼šç‚¹å‡»"å‘é€"æŒ‰é’®æˆ–æŒ‰Enteré”®
            3. **æ£€ç´¢åŠŸèƒ½**ï¼šå¯ç”¨RAGå¯ä»¥ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯
            4. **æŸ¥çœ‹å‚è€ƒ**ï¼šä¸‹æ–¹çš„è¡¨æ ¼æ˜¾ç¤ºæ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯
            5. **æ¸…ç©ºå¯¹è¯**ï¼šç‚¹å‡»"æ¸…ç©ºå¯¹è¯"æŒ‰é’®å¯ä»¥å¼€å§‹æ–°çš„å¯¹è¯
            6. **é‡è¦æé†’**ï¼šæ‰€æœ‰å›ç­”ä»…ä¾›å‚è€ƒï¼Œè¯·å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿ
            """)

    return demo


def main():
    print("å¯åŠ¨åŒ»ç–—é—®ç­”ç³»ç»Ÿ...")
    demo = create_interface()
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False
    )


if __name__ == "__main__":
    main()